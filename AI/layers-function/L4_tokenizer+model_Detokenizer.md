
### 模型的“编译前端”）

### 是什么

> **把人类语言“编译”为模型能计算的符号**

类比：

|世界|对应|
|---|---|
|C 代码|自然语言|
|编译器前端|Tokenizer|
|汇编 / opcode|Token ID|

### 关键事实

- ==tokenizer **不是通用的**==
    
- ==每个模型一套==
    
- ==tokenizer ≈ 模型的一部分==
    

---

## 第 5 层：模型本体（Qwen3）

### 是什么

> **一个巨大的概率函数**

形式上：

```
f(token_seq) → next_token_prob
```

### 模型不知道的事

- 不知道你是人
    
- 不知道 JSON
    
- 不知道 HTTP
    
- 不知道“你好”的含义
    

它只知道：

> “在统计上，下一个 token 是谁更合理”

---

## 第 6 层：Detokenizer（反编译）

### 是什么

> **把模型输出的整数翻译成人类语言**

没有它：

- 人类看到的只有数字
    

---

## 第 7 层：API 返回层

- Ollama 再次遵守 OpenAI 协议
    
- 把文本塞进 JSON
    
- 返回给 Dify
    

---

## 第 8 层：应用展示层（Dify）

### Dify 实际角色

- API Client
    
- Prompt 编排器
    
- UI 渲染器
    

### Dify 不做的事

- ❌ 不算模型
    
- ❌ 不处理 token
    
- ❌ 不管显卡
